{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead8f1a1-f9f3-4edf-8ca2-6857c7047cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - intel\n",
      " - conda-forge\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 24.3.0\n",
      "    latest version: 24.5.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/uec35afb227bdfda438e779741e943de/.conda/envs/llm\n",
      "\n",
      "  added / updated specs:\n",
      "    - python=3.11\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      intel/linux-64::_libgcc_mutex-0.1-conda_forge \n",
      "  _openmp_mutex      intel/linux-64::_openmp_mutex-4.5-2_gnu \n",
      "  bzip2              intel/linux-64::bzip2-1.0.8-hb9a14ef_9 \n",
      "  ca-certificates    intel/linux-64::ca-certificates-2024.6.2-hbcca054_0 \n",
      "  intelpython        intel/linux-64::intelpython-2024.2.0-0 \n",
      "  libexpat           intel/linux-64::libexpat-2.6.2-h59595ed_0 \n",
      "  libffi             intel/linux-64::libffi-3.4.2-h7f98852_5 \n",
      "  libgcc-ng          intel/linux-64::libgcc-ng-13.2.0-h77fa898_7 \n",
      "  libgomp            intel/linux-64::libgomp-13.2.0-h77fa898_7 \n",
      "  libnsl             intel/linux-64::libnsl-2.0.1-hd590300_0 \n",
      "  libsqlite          intel/linux-64::libsqlite-3.45.3-h2797004_0 \n",
      "  libuuid            intel/linux-64::libuuid-2.38.1-h0b41bf4_0 \n",
      "  libxcrypt          intel/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
      "  libzlib            intel/linux-64::libzlib-1.2.13-h4ab18f5_6 \n",
      "  ncurses            intel/linux-64::ncurses-6.5-h59595ed_0 \n",
      "  openssl            intel/linux-64::openssl-3.3.1-h4ab18f5_0 \n",
      "  pip                intel/noarch::pip-24.0-pyhd8ed1ab_0 \n",
      "  python             intel/linux-64::python-3.11.9-h2324612_2_cpython \n",
      "  readline           intel/linux-64::readline-8.2-h8228510_1 \n",
      "  setuptools         intel/noarch::setuptools-70.0.0-pyhd8ed1ab_0 \n",
      "  tk                 intel/linux-64::tk-8.6.13-noxft_h4845f30_101 \n",
      "  tzdata             intel/noarch::tzdata-2024a-h0c530f3_0 \n",
      "  wheel              intel/noarch::wheel-0.43.0-pyhd8ed1ab_1 \n",
      "  xz                 intel/linux-64::xz-5.2.8-h5eee18b_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "libffi-3.4.2         | 57 KB     |                                       |   0% \n",
      "_openmp_mutex-4.5    | 23 KB     |                                       |   0% \u001b[A\n",
      "\n",
      "_libgcc_mutex-0.1    | 3 KB      |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "_libgcc_mutex-0.1    | 3 KB      | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "libffi-3.4.2         | 57 KB     | ##################################### | 100% \u001b[A\u001b[A\n",
      "_openmp_mutex-4.5    | 23 KB     | ##################################### | 100% \u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate llm\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: ipex-llm[all] in ./.local/lib/python3.9/site-packages (2.1.0b20240623)\n",
      "Requirement already satisfied: py-cpuinfo in ./.local/lib/python3.9/site-packages (from ipex-llm[all]) (9.0.0)\n",
      "Requirement already satisfied: protobuf in ./.local/lib/python3.9/site-packages (from ipex-llm[all]) (5.27.1)\n",
      "Requirement already satisfied: mpmath==1.3.0 in ./.local/lib/python3.9/site-packages (from ipex-llm[all]) (1.3.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from ipex-llm[all]) (1.26.4)\n",
      "Collecting transformers==4.36.2 (from ipex-llm[all])\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.9/site-packages (from ipex-llm[all]) (0.2.0)\n",
      "Requirement already satisfied: tokenizers==0.15.2 in ./.local/lib/python3.9/site-packages (from ipex-llm[all]) (0.15.2)\n",
      "Requirement already satisfied: accelerate==0.23.0 in ./.local/lib/python3.9/site-packages (from ipex-llm[all]) (0.23.0)\n",
      "Requirement already satisfied: tabulate in ./.local/lib/python3.9/site-packages (from ipex-llm[all]) (0.9.0)\n",
      "Requirement already satisfied: intel-openmp in ./.local/lib/python3.9/site-packages (from ipex-llm[all]) (2024.2.0)\n",
      "Collecting torch==2.1.2+cpu (from ipex-llm[all])\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-2.1.2%2Bcpu-cp39-cp39-linux_x86_64.whl (184.9 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from accelerate==0.23.0->ipex-llm[all]) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from accelerate==0.23.0->ipex-llm[all]) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from accelerate==0.23.0->ipex-llm[all]) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in ./.local/lib/python3.9/site-packages (from accelerate==0.23.0->ipex-llm[all]) (0.23.4)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.9/site-packages (from torch==2.1.2+cpu->ipex-llm[all]) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.9/site-packages (from torch==2.1.2+cpu->ipex-llm[all]) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.9/site-packages (from torch==2.1.2+cpu->ipex-llm[all]) (1.12.1)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.9/site-packages (from torch==2.1.2+cpu->ipex-llm[all]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.9/site-packages (from torch==2.1.2+cpu->ipex-llm[all]) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.9/site-packages (from torch==2.1.2+cpu->ipex-llm[all]) (2024.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.9/site-packages (from transformers==4.36.2->ipex-llm[all]) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from transformers==4.36.2->ipex-llm[all]) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.9/site-packages (from transformers==4.36.2->ipex-llm[all]) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from transformers==4.36.2->ipex-llm[all]) (4.66.2)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in ./.local/lib/python3.9/site-packages (from intel-openmp->ipex-llm[all]) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.9/site-packages (from jinja2->torch==2.1.2+cpu->ipex-llm[all]) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->transformers==4.36.2->ipex-llm[all]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->transformers==4.36.2->ipex-llm[all]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->transformers==4.36.2->ipex-llm[all]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->transformers==4.36.2->ipex-llm[all]) (2024.2.2)\n",
      "Using cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torch, transformers\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.1\n",
      "    Uninstalling torch-2.3.1:\n",
      "      Successfully uninstalled torch-2.3.1\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/uec35afb227bdfda438e779741e943de/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.37.0\n",
      "    Uninstalling transformers-4.37.0:\n",
      "      Successfully uninstalled transformers-4.37.0\n",
      "\u001b[33m  WARNING: The script transformers-cli is installed in '/home/uec35afb227bdfda438e779741e943de/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed torch-2.1.2+cpu transformers-4.36.2\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers==4.37.0\n",
      "  Using cached transformers-4.37.0-py3-none-any.whl.metadata (129 kB)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.9/site-packages (from transformers==4.37.0) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.local/lib/python3.9/site-packages (from transformers==4.37.0) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from transformers==4.37.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from transformers==4.37.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from transformers==4.37.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.9/site-packages (from transformers==4.37.0) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from transformers==4.37.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.local/lib/python3.9/site-packages (from transformers==4.37.0) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.9/site-packages (from transformers==4.37.0) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from transformers==4.37.0) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->transformers==4.37.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->transformers==4.37.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->transformers==4.37.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->transformers==4.37.0) (2024.2.2)\n",
      "Using cached transformers-4.37.0-py3-none-any.whl (8.4 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "\u001b[33m  WARNING: The script transformers-cli is installed in '/home/uec35afb227bdfda438e779741e943de/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed transformers-4.37.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!conda create -n llm python=3.11 -y\n",
    "!conda activate llm\n",
    "!pip install --pre --upgrade ipex-llm[all] --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install transformers==4.37.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ce01f1-5ab1-4430-8f19-8214ea0eb5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: huggingface_hub[cli] in ./.local/lib/python3.9/site-packages (0.23.4)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.9/site-packages (from huggingface_hub[cli]) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.9/site-packages (from huggingface_hub[cli]) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from huggingface_hub[cli]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from huggingface_hub[cli]) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from huggingface_hub[cli]) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from huggingface_hub[cli]) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.9/site-packages (from huggingface_hub[cli]) (4.12.2)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in ./.local/lib/python3.9/site-packages (from huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in ./.local/lib/python3.9/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.42)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->huggingface_hub[cli]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->huggingface_hub[cli]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->huggingface_hub[cli]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->huggingface_hub[cli]) (2024.2.2)\n",
      "Requirement already satisfied: wcwidth in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43df7cc9-aba7-4412-a507-6fb2647fdecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd44a4c7be94d498b3d2d476a848409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "#Enter your Huggingface Access Token in the Text Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fda590f-ede4-4a59-aac5-61be2b728141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in ./.local/lib/python3.9/site-packages (2.1.2+cpu)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.9/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.9/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.9/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./.local/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -inja2 (/home/uec35afb227bdfda438e779741e943de/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mhooray\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall torchvision -y\n",
    "!pip install torch\n",
    "#!pip install requirements.txt\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "print(\"hooray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc9fcfba-4b76-478a-b105-a994e4297d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d333f3790f4beaad072c00fc9d0d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Meta Llama 3? A Meta AI model that generates high-quality images and videos\n",
      "Meta Llama 3 is a powerful AI model developed by Meta AI that can generate high-quality images and videos. It's a variant of the\n"
     ]
    }
   ],
   "source": [
    "#=================================ONE RESPONSE=========================\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Define model name and token directly\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "HUGGINGFACE_TOKEN = '<your-huggingface-access-token-here>' \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=HUGGINGFACE_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    token=HUGGINGFACE_TOKEN\n",
    ")\n",
    "\n",
    "# Add a padding token if not already present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move model to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Test the model\n",
    "prompt = \"What is Meta Llama 3?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "# Provide attention mask and handle pad token id\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"], \n",
    "    attention_mask=attention_mask, \n",
    "    max_length=50, \n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "071e2949-4deb-4b72-ad20-442aaa75c029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bed2dd41bf14e02bc8455058f3a8067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1:\n",
      " What is Meta LLama 3\n",
      "Response 1:\n",
      " What is Meta LLama 3.0?\n",
      "Meta LLaMA 3.0 is a new AI model developed by Meta AI that is designed to generate human-like text responses to user input. It is a large language model that is trained\n",
      " \n",
      "Prompt 2:\n",
      " What is Intel IPEX LLM\n",
      "Response 2:\n",
      " What is Intel IPEX LLM?\n",
      "Intel IPEX (Integrated Processor Extensions) is a set of technologies developed by Intel to enhance the performance and efficiency of their processors. LLM (Large Language Model) is a specific type of AI\n"
     ]
    }
   ],
   "source": [
    "##=================================RAW RESPONSES=========================\n",
    "#Key Observations:\n",
    "# 1. Question is repeated in the response\n",
    "# 2. Response does not end in a full sentence\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Define model name and token directly\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "HUGGINGFACE_TOKEN = '<your-huggingface-access-token-here>' \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=HUGGINGFACE_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    token=HUGGINGFACE_TOKEN\n",
    ")\n",
    "\n",
    "# Add a padding token if not already present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move model to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#Test the model\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        attention_mask=attention_mask, \n",
    "        max_length=50, \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# First prompt\n",
    "prompt1 = \"What is Meta LLama 3\"\n",
    "response1 = generate_response(prompt1)\n",
    "print(\"Prompt 1:\\n\", prompt1)\n",
    "print(\"Response 1:\\n\", response1)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "# Second prompt\n",
    "prompt2 = \"What is Intel IPEX LLM\"\n",
    "response2 = generate_response(prompt2)\n",
    "print(\"Prompt 2:\\n\", prompt2)\n",
    "print(\"Response 2:\\n\", response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6f78f98-5484-44cb-8de7-300d52abc980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d623f5df558b489c82fef60856939b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: What is Meta LLama 3\n",
      "Response 1: What is Meta LLama 3D?\n",
      "Meta LLaMA 3D is a new AI model developed by Meta AI that uses a combination of text-to-image generation and 3D rendering to create photorealistic 3D models from text descriptions. The model is trained on a large dataset of text and image pairs, and can generate 3D models that are indistinguishable from those created by a human artist.\n",
      "\n",
      "Meta LLaMA 3D is designed to be used for\n",
      " \n",
      "Prompt 2: What is Intel IPEX LLM\n",
      "Response 2: What is Intel IPEX LLM?\n",
      "Intel IPEX LLM is a machine learning model that is designed to be a lightweight and efficient language model. It is a type of transformer-based model that is specifically designed for edge AI applications, such as smart home devices, IoT devices, and other resource-constrained devices.\n",
      "\n",
      "IPEX stands for \"Intel Portable Experiential eXtension\" and it is a set of pre-trained language models that are designed to be portable and\n"
     ]
    }
   ],
   "source": [
    "#============================RESPONSE ADJUSTED WITH MORE SPECIFIC PARAMETERS===============================\n",
    "#Key Observations: \n",
    "# 1. Question is now no longer repeated.\n",
    "# 2. Response still doesn't end in a full sentence. \n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Define model name and token directly\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "HUGGINGFACE_TOKEN = '<your-huggingface-access-token-here>' \n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=HUGGINGFACE_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    token=HUGGINGFACE_TOKEN\n",
    ")\n",
    "\n",
    "# Add a padding token if not already present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move model to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def generate_response(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        attention_mask=attention_mask, \n",
    "        max_length=max_length, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test the model with two prompts\n",
    "prompt1 = \"What is Meta LLama 3\"\n",
    "response1 = generate_response(prompt1)\n",
    "print(\"Prompt 1:\", prompt1)\n",
    "print(\"Response 1:\", response1)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "prompt2 = \"What is Intel IPEX LLM\"\n",
    "response2 = generate_response(prompt2)\n",
    "print(\"Prompt 2:\", prompt2)\n",
    "print(\"Response 2:\", response2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe703148-cb53-42f1-a3b1-f4ba762ddb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3f9353d0044a1097dc9b7db65cd9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: What is Meta LLama 3\n",
      "Response 1: What is Meta LLama 3.0?\n",
      "Meta LLaMA 3.0 is an AI model developed by Meta AI that is designed to generate human-like text responses to user input. It is a type of language model that uses a combination of natural language processing (NLP) and machine learning algorithms to understand and respond to user queries.\n",
      "Meta LLaMA 3.\n",
      " \n",
      "Prompt 2: What is Intel IPEX LLM\n",
      "Response 2: What is Intel IPEX LLM?\n",
      "Intel IPEX LLM (Logic, Language, and Memory) is a deep learning-based language model developed by Intel. It is designed to improve the performance of natural language processing (NLP) tasks, such as language translation, text summarization, and question answering.\n",
      "\n",
      "IPEX LLM is built on top of the Intel Nervana Neon deep learning framework and uses a combination of transformer and recurrent neural network (RNN) architectures.\n"
     ]
    }
   ],
   "source": [
    "#============================RESPONSE TUNED TO END IN FULL SENTENCE===============================\n",
    "# Key Observations:\n",
    "# 1. Now Question is not repeated in the response.\n",
    "# 2. Now sentence ends in a full sentence.\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Define model name and token directly\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "HUGGINGFACE_TOKEN = '<your-huggingface-access-token-here>' \n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=HUGGINGFACE_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    token=HUGGINGFACE_TOKEN\n",
    ")\n",
    "\n",
    "# Add a padding token if not already present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move model to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def generate_response(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        attention_mask=attention_mask, \n",
    "        max_length=max_length, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Ensure the response ends with a complete sentence\n",
    "    end_punctuation = {'.', '!', '?'}\n",
    "    for i in range(len(response)-1, 0, -1):\n",
    "        if response[i] in end_punctuation:\n",
    "            return response[:i+1]\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the model with two prompts\n",
    "prompt1 = \"What is Meta LLama 3\"\n",
    "response1 = generate_response(prompt1)\n",
    "print(\"Prompt 1:\", prompt1)\n",
    "print(\"Response 1:\", response1)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "prompt2 = \"What is Intel IPEX LLM\"\n",
    "response2 = generate_response(prompt2)\n",
    "print(\"Prompt 2:\", prompt2)\n",
    "print(\"Response 2:\", response2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22905ea8-6d7a-46d5-9abe-30c4f70865bf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
